## Who am I and Why am I Doing This

Hello, my name is kymb0 and the universe is now accelerating towards AI adoption at a rate faster than my grandmother can demolish a bottle of cheap champagne.
This, in turn, has made me a strange mix of curious, furious, and anxious. I knew I would have to stand up and ride the wave or be dumped to the depths in the Smithsonian.

I dd not feel comfortable with simply learning the new and emerging attack vectors, so I began chewing through the `Fundamentals of AI` Module on the ![Hack the Box Academy](https://academy.hackthebox.com). While I was doing this, I realised that while this was all well and good - there was an exactly 4.5% chance I would retain any of it, as I can barely retain which street I live on as it is.

Thus, I thought to myself as I sipped my 4th coffee within 30 minutes after waking at 4am, I MUST yield another blog.

## What Will We Be Doing?

First and most importantly, after a long history of leveraging Adventure Time memes to convey what I often fail to articulate, we will be switching to Supernatural memes - I myself do not consume much television, however I find this to be a tremendous show and you must watch the entire 15 seasons before proceeding.

The reason we are switching to Supernatural is because in this blog series, we are going to use the power of `AI`  to assist Sam and Dean Winchester in their quest to hunt the Supernatural.

### HOW???

We are going to create a dataset of sensor readings (inputs) and Supernatural Severity outcome (a numeric target). For the first entry in this series will use regression – one of the simplest forms of machine learning – to build a model that guesses the breach intensity from the sensor inputs. This first entry in our series will explain linear regression from the ground up. I have no prior ML or math background, so every concept will be explained in simple terms.  
By the end, you’ll know how to train a linear regression model using Python, interpret its output (coefficients, residuals, etc.), understand its assumptions, and see how this knowledge sets the stage for more advanced topics.

<put markdown expansion box here>  
TL;DR: We’re using a simple machine learning technique (linear regression) to fit a straight line through breach data points and predict a numeric outcome (breach intensity). Think of it like a basic AI that weighs security sensor inputs to estimate how severe a breach will be. We’ll walk through the concept, code, visualization, and interpretation step by step. I am learning this as we go, so hopefully I do not lead you astray. 

By framing cybersecurity detection through the lens of supernatural threat hunting, we build a bridge between unfamiliar technical ideas and something inherently visual and pattern-driven.

If it’s good enough for the boys (Sam and Dean Winchester), it’s good enough for us.

## Getting Started

So, what are we actually going to do with all this?

We're going to build some small models to demonstrate the fundamentals — not models to do your homework, or vibe-code the next social media dopamine dump extravaganza. Just simple systems that look at patterns in data and say, “Yeah... something's not right here,” or “These data points feel connected — and that usually means <this>.”

And we'll learn how they work, why they break, and how to think like someone who could either trust or tamper with them.

Let’s start simple: teaching a machine to draw a straight line through chaos, to do this we will use Linear Regression.


## What is Linear Regression "Using data to Predict the future with Lines"

Regression is conceptually tasked with predicting a numeric outcome, while __linear regression__ is a sub-approach that will assume the relationship between inputs/datapoints is either a straight line or a "flat plane in multiple dimensions". For the purpose of linear regression, the assumption will be the former, with the hope this line can then be used to predict new points we haven’t seen. Flat planes being explored in a later episode when we cover Support Vector Machines (SVMs)

__In linear regression, an **intercept** is the baseline outcome — the value of `y` when all inputs are zero__  
Think of it like the background danger level when no obvious signs are present. (Even if no EMF spike or sulfur trace is detected, there's still a small chance something is lurking depending on certain factors such as previous_sightings)  

__A **coefficient** is how strongly an input affects the prediction__  
For example:   
- If sulfur_presence has a coefficient of `+4.2`, every unit increase makes the situation 4.2 units more dangerous.
- If angel_energy has a coefficient of `-2.1`, higher spiritual residue *reduces* the danger (maybe Cas has already been here).  


### A Poor Example of the Math

```
In its simplest form, a linear regression model looks like:

    y = m * x + c

Where:
- `y` is the value we want to predict (like supernatural_score)
- `x` is the input (like environmental_anomalies)
- `m` is the slope (how much y changes for each unit of x)
- `c` is the intercept (what y would be if x were zero)

If `m = 2` and `c = 5`, and we see `x = 3`, then:

    y = 2 * 3 + 5 = 11

Meaning, based on our model, we'd predict a supernatural score of 11.

```



## What is Ordinary Least Squares "Tuning for Prediction Accuracy"

Once we decide we’re fitting a line, the next question is: which line?

There are infinite lines you could draw through the mad cacophony of gathered data. Some will overestimate, some will undershoot, and some will be outright cursed. What we want is the best possible line, one that minimizes how wrong we are — on average — across all predictions.

That’s where __Ordinary Least Squares (OLS)__ comes in.

OLS is the method linear regression uses to pick the optimal line. It works like this:

- For every data point in the training set, the model predicts a value.

- It then compares that guess to the real, known outcome — the difference between them becomes the **__Residual__**.

- To prevent negative and positive errors from cancelling each other out it squares each residual to force a positive number (eg on data point has a Residual of `10` and another has a Residual of a `-10`).

- It then sums all these squared errors into one value.

- Finally, it adjusts the slope and position of the line (technically, the coefficients and intercept) to find the configuration that produces the lowest total error.

This process is literally what “least squares” means: the line that produces the smallest possible total squared error gets chosen as the model.

In other words: OLS tunes the line until it hugs the center of your data cloud as tightly as possible — not perfectly, but optimally.

### Another Poor Example of the Math
```
Suppose our model predicts:

| Actual y | Predicted y | Residual (Actual - Predicted) |
|----------|-------------|-------------------------------|
| 17       | 15           | +2                            |
| 26       | 28           | -2                            |
| 14       | 12           | +2                            |
| 23       | 24           | -1                            |

Then we square the residuals:

(+2)^2 = 4  
(-2)^2 = 4  
(+2)^2 = 4  
(-1)^2 = 1  

Sum of squared errors = 4 + 4 + 4 + 1 = **13**

OLS tries to tune the model (adjust m and c) to make this total error **as small as possible**.
```

## How Can we us this to help Sam and Dean

Demon Detection Analogy

Imagine a field log of ritual indicators: resonance spikes, temperature shifts, sigil interference. A linear model tries to draw a predictive line: "Each glyph pulse adds 2.4 breach units; low-frequency chanting drops it by 1.0."

It’s not magic. It’s math. We're modeling reality to predict something dangerous — and what you learn from those coefficients may help you defend... or exploit depending on who or what you are.

## Running Our Own Model and Testing

Now let's run some contrived examples to visually demonstrate all this mumbo-jumbo.

First, install ![JupyterLab](https://jupyter.org)
`pip install jupyterlab`
Then run with `jupyter lab`


```
# Import libraries
import pandas as pd
import numpy as np

# Set random seed for repeatability
np.random.seed(42)

# Create synthetic features
environmental_anomalies = np.random.randint(0, 10, size=100)  # EMF spikes, localized temp drops
entity_signatures = np.random.randint(0, 5, size=100)         # Generic traces (claw marks, sigil damage)
sulfur_presence = np.random.randint(0, 3, size=100)           # Specific sulfur readings (rare but heavy indicator)
angel_energy = np.random.normal(5.0, 1.5, size=100)           # Higher angel energy reduces threat

# Create target variable (supernatural severity)
# Linear combination + noise
supernatural_severity = (
    3.5 * environmental_anomalies +    # environmental anomolies increase danger
    2.5 * entity_signatures +           # minor evidence increases danger
    6.0 * sulfur_presence +             # sulfur traces heavily increase danger
    -4.0 * angel_energy +               # angelic energy lowers danger
    np.random.normal(0, 5, size=100)    # random environmental noise
)

# Bundle into a DataFrame
data = pd.DataFrame({
    'environmental_anomalies': environmental_anomalies,
    'entity_signatures': entity_signatures,
    'sulfur_presence': sulfur_presence,
    'angel_energy': angel_energy,
    'supernatural_severity': supernatural_severity
})

# Save dataset to CSV (optional)
data.to_csv('supernatural_dataset.csv', index=False)

# Quick peek at the data
print(data.head())

```
